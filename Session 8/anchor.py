# -*- coding: utf-8 -*-
"""anchor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15GYxZUuWonBKmiVa6JPJk-FpFIrut0Tz

Reference: Dive into Deep Learnin Book
"""

!pip install tensorflow==2.12.0 tensorflow-probability==0.20.0

!pip install d2l==1.0.3

"""# Anchor Boxes


Object detection algorithms usually
sample a large number of regions in the input image, determine whether these regions contain
objects of interest, and adjust the boundaries
of the regions so as to predict the
*ground-truth bounding boxes*
of the objects more accurately.
Different models may adopt
different region sampling schemes.
Here we introduce one of such methods:
it generates multiple bounding boxes with varying scales and aspect ratios centered on each pixel.
These bounding boxes are called *anchor boxes*.
We will design an object detection model
based on anchor boxes.

First, let's modify the printing accuracy
just for more concise outputs.

"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import torch
from d2l import torch as d2l

torch.set_printoptions(2)  # Simplify printing accuracy

"""## Generating Multiple Anchor Boxes

Suppose that the input image has a height of $h$ and width of $w$.
We generate anchor boxes with different shapes centered on each pixel of the image.
Let the *scale* be $s\in (0, 1]$ and
the *aspect ratio* (ratio of width to height) is $r > 0$.
Then [**the width and height of the anchor box are $ws\sqrt{r}$ and $hs/\sqrt{r}$, respectively.**]
Note that when the center position is given, an anchor box with known width and height is determined.

To generate multiple anchor boxes with different shapes,
let's set a series of scales
$s_1,\ldots, s_n$ and
a series of aspect ratios $r_1,\ldots, r_m$.
When using all the combinations of these scales and aspect ratios with each pixel as the center,
the input image will have a total of $whnm$ anchor boxes. Although these anchor boxes may cover all the
ground-truth bounding boxes, the computational complexity is easily too high.
In practice,
we can only (**consider those combinations
containing**) $s_1$ or $r_1$:

(**$$(s_1, r_1), (s_1, r_2), \ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \ldots, (s_n, r_1).$$**)

That is to say, the number of anchor boxes centered on the same pixel is $n+m-1$. For the entire input image, we will generate a total of $wh(n+m-1)$ anchor boxes.

The above method of generating anchor boxes is implemented in the following `multibox_prior` function. We specify the input image, a list of scales, and a list of aspect ratios, then this function will return all the anchor boxes.

"""

def multibox_prior(data, sizes, ratios):
    """Generate anchor boxes with different shapes centered on each pixel."""
    # data.device:
    # Gets the device (e.g., cpu, cuda:0, etc.) where the tensor data is currently located.
    # torch.tensor(..., device=device):
    # Creates new tensors (size_tensor, ratio_tensor) directly on the same device as data.
###############################
#Step 1: Extract spatial info and prepare device
    #Extract the height and width of the input tensor (feature map).
    in_height, in_width = data.shape[-2:]
    #device: Get the current device (cpu, cuda:0, etc.) where data resides.
    # num_sizes and num_ratios: The number of scales and aspect ratios to use for anchors.
    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)
    #Total number of anchor boxes per pixel = all combinations of sizes and ratios, but removes a duplicate (sizes[0] with ratios[0]).
###############################
#Step 2: Tensor conversion and counts
    boxes_per_pixel = (num_sizes + num_ratios - 1)
    #Convert sizes and ratios to tensors and move them to the same device as the data.
    size_tensor = torch.tensor(sizes, device=device)
    ratio_tensor = torch.tensor(ratios, device=device)
###############################
#Step 3: Determine center positions
    # Offsets are required to move the anchor to the center of a pixel. Since
    # a pixel has height=1 and width=1, we choose to offset our centers by 0.5
    #Offset centers by 0.5 to center them within each pixel cell (since pixels are 1 unit wide/high).
    offset_h, offset_w = 0.5, 0.5
    #scale pixel height/width so positions are between 0 and 1.
    steps_h = 1.0 / in_height  # Scaled steps in y axis
    steps_w = 1.0 / in_width  # Scaled steps in x axis

    # Generate all center points for the anchor boxes
    #Get all y- and x-coordinates for pixel centers on the normalized grid.
    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h
    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w
    # Use meshgrid to form a full grid of all center positions.
    # Flatten to 1D to make anchor generation easier.
    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')
    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)
###############################
#Step 4: Calculate anchor shapes (height/width)
#We want to generate a set of anchor box shapes (width and height) for each center point on a feature map.
#We are given:
# A list of sizes (scales, e.g., [0.2, 0.4, 0.6])
# A list of ratios (aspect ratios, e.g., [1, 2, 0.5])
# And we want to combine them in the following way:
# The first size (i.e., sizes[0]) is combined with each ratio.
# The remaining sizes[1:] are combined with only the first ratio (usually 1, i.e., square boxes).
# This avoids unnecessary redundancy (e.g., multiple boxes with very similar shape).


    # Generate `boxes_per_pixel` number of heights and widths that are later
    # used to create anchor box corner coordinates (xmin, xmax, ymin, ymax)
    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),
                   sizes[0] * torch.sqrt(ratio_tensor[1:])))\
                   * in_height / in_width  # Handle rectangular inputs
    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),
                   sizes[0] / torch.sqrt(ratio_tensor[1:])))
    # Divide by 2 to get half height and half width
    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(
                                        in_height * in_width, 1) / 2

    # Each center point will have `boxes_per_pixel` number of anchor boxes, so
    # generate a grid of all anchor box centers with `boxes_per_pixel` repeats
    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],
                dim=1).repeat_interleave(boxes_per_pixel, dim=0)
    output = out_grid + anchor_manipulations
    return output.unsqueeze(0)

"""We can see that [**the shape of the returned anchor box variable `Y`**] is
(batch size, number of anchor boxes, 4).
The last dimension of size 4 corresponds to the coordinates of each anchor box, specifically in this order:
(x_min, y_min, x_max, y_max)
These are scaled coordinates:
x_min, x_max are in the range [0,1] with respect to image width
y_min, y_max are in the range [0,1] with respect to image height

So each anchor box is defined by:
The top-left corner: (x_min, y_min)
The bottom-right corner: (x_max, y_max)
"""

img = d2l.plt.imread('./catdog.jpg')
h, w = img.shape[:2]

print(h, w)
X = torch.rand(size=(1, 3, h, w))  # Construct input data
Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])
Y.shape

"""After changing the shape of the anchor box variable `Y` to (image height, image width, number of anchor boxes centered on the same pixel, 4),
we can obtain all the anchor boxes centered on a specified pixel position.
In the following,
we [**access the first anchor box centered on (250, 250)**]. It has four elements: the $(x, y)$-axis coordinates at the upper-left corner and the $(x, y)$-axis coordinates at the lower-right corner of the anchor box.
The coordinate values of both axes
are divided by the width and height of the image, respectively.

"""

boxes = Y.reshape(h, w, 5, 4)
boxes[250, 250, 0, :]

"""In order to [**show all the anchor boxes centered on one pixel in the image**],
we define the following `show_bboxes` function to draw multiple bounding boxes on the image.

"""

# axes: A matplotlib.axes.Axes object, used to draw shapes on an image.
# bboxes: A list or tensor of bounding boxes, each in (xmin, ymin, xmax, ymax) format.
# labels: Optional text labels for each box.
# colors: Optional list of colors for the boxes.
def show_bboxes(axes, bboxes, labels=None, colors=None):
    """Show bounding boxes."""
    # Ensures that labels and colors are always in list format:
    # If None, replace with default_values
    # If a single value (e.g. a string or number), wrap it in a list
    # If already a list/tuple, return as-is
    # This allows the code to uniformly handle single or multiple labels/colors.
    def make_list(obj, default_values=None):
        if obj is None:
            obj = default_values
        elif not isinstance(obj, (list, tuple)):
            obj = [obj]
        return obj

    labels = make_list(labels)
    colors = make_list(colors, ['b', 'g', 'r', 'm', 'c'])
    # Iterate over all bounding boxes and their index i.
    for i, bbox in enumerate(bboxes):
        color = colors[i % len(colors)]
        #Converts a PyTorch bbox tensor to a NumPy array, then to a matplotlib.patches.Rectangle via d2l.bbox_to_rect.
        #This function expects (xmin, ymin, xmax, ymax) and color, and returns a rectangle object.
        rect = d2l.bbox_to_rect(bbox.detach().numpy(), color)
        #Adds the rectangle to the plot (image).
        axes.add_patch(rect)
        #Only draw label if it exists and is within bounds of the labels list.
        if labels and len(labels) > i:
           #Sets text color to black ('k') if the box is white ('w'), otherwise white.
            text_color = 'k' if color == 'w' else 'w'
            #Places a text label at the top-left corner of the rectangle.
            #va='center', ha='center': vertically and horizontally center the text.
            #fontsize=9: sets small font size.
            #bbox=...: draws a colored background behind the label text, matching the rectangle color.
            axes.text(rect.xy[0], rect.xy[1], labels[i],
                      va='center', ha='center', fontsize=9, color=text_color,
                      bbox=dict(facecolor=color, lw=0))

"""As we just saw, the coordinate values of the $x$ and $y$ axes in the variable `boxes` have been divided by the width and height of the image, respectively.
When drawing anchor boxes,
we need to restore their original coordinate values;
thus, we define variable `bbox_scale` below.
Now, we can draw all the anchor boxes centered on (250, 250) in the image.
As you can see, the blue anchor box with a scale of 0.75 and an aspect ratio of 1 well
surrounds the dog in the image.

"""

d2l.set_figsize()
bbox_scale = torch.tensor((w, h, w, h))
fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,
            ['s=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2',
             's=0.75, r=0.5'])

"""## [**Intersection over Union (IoU)**]

We just mentioned that an anchor box "well" surrounds the dog in the image.
If the ground-truth bounding box of the object is known, how can "well" here be quantified?
Intuitively, we can measure the similarity between
the anchor box and the ground-truth bounding box.
We know that the *Jaccard index* can measure the similarity between two sets. Given sets $\mathcal{A}$ and $\mathcal{B}$, their Jaccard index is the size of their intersection divided by the size of their union:

$$J(\mathcal{A},\mathcal{B}) = \frac{\left|\mathcal{A} \cap \mathcal{B}\right|}{\left| \mathcal{A} \cup \mathcal{B}\right|}.$$


In fact, we can consider the pixel area of any bounding box as a set of pixels.
In this way, we can measure the similarity of the two bounding boxes by the Jaccard index of their pixel sets. For two bounding boxes, we usually refer their Jaccard index as *intersection over union* (*IoU*), which is the ratio of their intersection area to their union area, as shown in :numref:`fig_iou`.
The range of an IoU is between 0 and 1:
0 means that two bounding boxes do not overlap at all,
while 1 indicates that the two bounding boxes are equal.

![IoU is the ratio of the intersection area to the union area of two bounding boxes.](http://d2l.ai/_images/iou.svg)
:label:`fig_iou`

For the remainder of this section, we will use IoU to measure the similarity between anchor boxes and ground-truth bounding boxes, and between different anchor boxes.
Given two lists of anchor or bounding boxes,
the following `box_iou` computes their pairwise IoU
across these two lists.

"""

def box_iou(boxes1, boxes2):
    """Compute pairwise IoU across two lists of anchor or bounding boxes."""
    #Anonymous function (lambda) to compute area of each box.
    #x_max - x_min gives width
    #y_max - y_min gives height
    #This assumes each box is in format: (x_min, y_min, x_max, y_max)
    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *
                              (boxes[:, 3] - boxes[:, 1]))
    # Shape of `boxes1`, `boxes2`, `areas1`, `areas2`: (no. of boxes1, 4),
    # (no. of boxes2, 4), (no. of boxes1,), (no. of boxes2,)
    areas1 = box_area(boxes1)
    areas2 = box_area(boxes2)
    # Shape of `inter_upperlefts`, `inter_lowerrights`, `inters`: (no. of
    # boxes1, no. of boxes2, 2)

   # boxes1[:, None, :2]: adds a dimension so it becomes shape (n1, 1, 2) to broadcast over boxes2
    #boxes2[:, :2]: shape (n2, 2)
    #inter_upperlefts: shape (n1, n2, 2) — for each pair of boxes, computes the top-left corner of the overlapping region.
    #Similarly, inter_lowerrights: computes the bottom-right corner of the overlapping region.
    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])
    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])

    #Elementwise width and height of the intersection box.
    #clamp(min=0) ensures negative values (non-overlapping boxes) become zero.
    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)
    # Shape of `inter_areas` and `union_areas`: (no. of boxes1, no. of boxes2)
    #Width × Height = Area of intersection
    inter_areas = inters[:, :, 0] * inters[:, :, 1]
    #Union = Area1 + Area2 − Intersection
    union_areas = areas1[:, None] + areas2 - inter_areas
    return inter_areas / union_areas

"""## Labeling Anchor Boxes in Training Data
:label:`subsec_labeling-anchor-boxes`


In a training dataset,
we consider each anchor box as a training example.
In order to train an object detection model,
we need *class* and *offset* labels for each anchor box,
where the former is
the class of the object relevant to the anchor box
and the latter is the offset
of the ground-truth bounding box relative to the anchor box.
During the prediction,
for each image
we generate multiple anchor boxes,
predict classes and offsets for all the anchor boxes,
adjust their positions according to the predicted offsets to obtain the predicted bounding boxes,
and finally only output those
predicted bounding boxes that satisfy certain criteria.


As we know, an object detection training set
comes with labels for
locations of *ground-truth bounding boxes*
and classes of their surrounded objects.
To label any generated *anchor box*,
we refer to the labeled
location and class of its *assigned* ground-truth bounding box that is closest to the anchor box.
In the following,
we describe an algorithm for assigning
closest ground-truth bounding boxes to anchor boxes.

### [**Assigning Ground-Truth Bounding Boxes to Anchor Boxes**]

Given an image,
suppose that the anchor boxes are $A_1, A_2, \ldots, A_{n_a}$ and the ground-truth bounding boxes are $B_1, B_2, \ldots, B_{n_b}$, where $n_a \geq n_b$.
Let's define a matrix $\mathbf{X} \in \mathbb{R}^{n_a \times n_b}$, whose element $x_{ij}$ in the $i^\textrm{th}$ row and $j^\textrm{th}$ column is the IoU of the anchor box $A_i$ and the ground-truth bounding box $B_j$. The algorithm consists of the following steps:

1. Find the largest element in matrix $\mathbf{X}$ and denote its row and column indices as $i_1$ and $j_1$, respectively. Then the ground-truth bounding box $B_{j_1}$ is assigned to the anchor box $A_{i_1}$. This is quite intuitive because $A_{i_1}$ and $B_{j_1}$ are the closest among all the pairs of anchor boxes and ground-truth bounding boxes. After the first assignment, discard all the elements in the ${i_1}^\textrm{th}$ row and the ${j_1}^\textrm{th}$ column in matrix $\mathbf{X}$.
1. Find the largest of the remaining elements in matrix $\mathbf{X}$ and denote its row and column indices as $i_2$ and $j_2$, respectively. We assign ground-truth bounding box $B_{j_2}$ to anchor box $A_{i_2}$ and discard all the elements in the ${i_2}^\textrm{th}$ row and the ${j_2}^\textrm{th}$ column in matrix $\mathbf{X}$.
1. At this point, elements in two rows and two columns in  matrix $\mathbf{X}$ have been discarded. We proceed until all elements in $n_b$ columns in matrix $\mathbf{X}$ are discarded. At this time, we have assigned a ground-truth bounding box to each of $n_b$ anchor boxes.
1. Only traverse through the remaining $n_a - n_b$ anchor boxes. For example, given any anchor box $A_i$, find the ground-truth bounding box $B_j$ with the largest IoU with $A_i$ throughout the $i^\textrm{th}$ row of matrix $\mathbf{X}$, and assign $B_j$ to $A_i$ only if this IoU is greater than a predefined threshold.

Let's illustrate the above algorithm using a concrete
example.
As shown in :numref:`fig_anchor_label` (left), assuming that the maximum value in matrix $\mathbf{X}$ is $x_{23}$, we assign the ground-truth bounding box $B_3$ to the anchor box $A_2$.
Then, we discard all the elements in row 2 and column 3 of the matrix, find the largest $x_{71}$ in the remaining  elements (shaded area), and assign the ground-truth bounding box $B_1$ to the anchor box $A_7$.
Next, as shown in :numref:`fig_anchor_label` (middle), discard all the elements in row 7 and column 1 of the matrix, find the largest $x_{54}$ in the remaining  elements (shaded area), and assign the ground-truth bounding box $B_4$ to the anchor box $A_5$.
Finally, as shown in :numref:`fig_anchor_label` (right), discard all the elements in row 5 and column 4 of the matrix, find the largest $x_{92}$ in the remaining elements (shaded area), and assign the ground-truth bounding box $B_2$ to the anchor box $A_9$.
After that, we only need to traverse through
the remaining anchor boxes $A_1, A_3, A_4, A_6, A_8$ and determine whether to assign them ground-truth bounding boxes according to the threshold.

![Assigning ground-truth bounding boxes to anchor boxes.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/anchor-label.svg?raw=1)
:label:`fig_anchor_label`

This algorithm is implemented in the following `assign_anchor_to_bbox` function.

"""

def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):
    """Assign closest ground-truth bounding boxes to anchor boxes."""
   # ground_truth: Tensor of shape (n_gt, 4) — Ground-truth boxes (corner format)
   # anchors: Tensor of shape (n_anchors, 4) — Anchor boxes to assign
   # device: CPU or GPU where tensors are stored
   # iou_threshold: IoU threshold to decide if an anchor is close enough to a GT box

    #Count how many anchors and GT boxes we have.
    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]

    # Compute the pairwise IoU matrix jaccard of shape (num_anchors, num_gt_boxes)
    # Entry [i, j] is the IoU between anchor i and GT box j.
    # Element x_ij in the i-th row and j-th column is the IoU of the anchor
    # box i and the ground-truth bounding box j
    jaccard = box_iou(anchors, ground_truth)
    # Initialize the tensor to hold the assigned ground-truth bounding box for
    # each anchor
   # Start with all anchors unassigned: -1 means "no match"
    #Later, this will store the index of the GT box assigned to each anchor.
    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,
                                  device=device)
    # Assign ground-truth bounding boxes according to the threshold
    # For each anchor, get:
    # max_ious: highest IoU over all GT boxes
    # indices: index of the GT box with that highest IoU
    max_ious, indices = torch.max(jaccard, dim=1)

    # For all anchors whose highest IoU is above the threshold:
    # Assign the GT box index to them
    anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)
    box_j = indices[max_ious >= iou_threshold]
    anchors_bbox_map[anc_i] = box_j

    # These are dummy values used to zero out columns and rows (effectively discarding them) once assigned.
    col_discard = torch.full((num_anchors,), -1)
    row_discard = torch.full((num_gt_boxes,), -1)


    for _ in range(num_gt_boxes):
        #We find the largest IoU in the entire matrix
        max_idx = torch.argmax(jaccard)  # Find the largest IoU. This gives you a 1D index into the flattened version of the 2D tensor

        # Convert the 1D index back to 2D: max_idx = anc_idx * num_gt_boxes + box_idx
        box_idx = (max_idx % num_gt_boxes).long()
        anc_idx = (max_idx / num_gt_boxes).long()

        #Force assignment: GT box box_idx is assigned to anchor anc_idx
        anchors_bbox_map[anc_idx] = box_idx

        # Discard this GT box and this anchor from future iterations by filling its row and column with dummy
        # values (-1), ensuring no double assignment.

        jaccard[:, box_idx] = col_discard
        jaccard[anc_idx, :] = row_discard
    return anchors_bbox_map

"""### Labeling Classes and Offsets

Now we can label the class and offset for each anchor box. Suppose that an anchor box $A$ is assigned
a ground-truth bounding box $B$.
On the one hand,
the class of the anchor box $A$ will be
labeled as that of $B$.
On the other hand,
the offset of the anchor box $A$
will be labeled according to the
relative position between
the central coordinates of $B$ and $A$
together with the relative size between
these two boxes.
Given varying
positions and sizes of different boxes in the dataset,
we can apply transformations
to those relative positions and sizes
that may lead to
more uniformly distributed offsets
that are easier to fit.
Here we describe a common transformation.
[**Given the central coordinates of $A$ and $B$ as $(x_a, y_a)$ and $(x_b, y_b)$,
their widths as $w_a$ and $w_b$,
and their heights as $h_a$ and $h_b$, respectively.
We may label the offset of $A$ as

$$\left( \frac{ \frac{x_b - x_a}{w_a} - \mu_x }{\sigma_x},
\frac{ \frac{y_b - y_a}{h_a} - \mu_y }{\sigma_y},
\frac{ \log \frac{w_b}{w_a} - \mu_w }{\sigma_w},
\frac{ \log \frac{h_b}{h_a} - \mu_h }{\sigma_h}\right),$$
**]
where default values of the constants are $\mu_x = \mu_y = \mu_w = \mu_h = 0, \sigma_x=\sigma_y=0.1$, and $\sigma_w=\sigma_h=0.2$.
This transformation is implemented below in the `offset_boxes` function.

"""

def offset_boxes(anchors, assigned_bb, eps=1e-6):
    """Transform for anchor box offsets."""
    c_anc = d2l.box_corner_to_center(anchors)
    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)
    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]
    offset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])
    offset = torch.cat([offset_xy, offset_wh], axis=1)
    return offset

"""If an anchor box is not assigned a ground-truth bounding box, we just label the class of the anchor box as "background".
Anchor boxes whose classes are background are often referred to as *negative* anchor boxes,
and the rest are called *positive* anchor boxes.
We implement the following `multibox_target` function
to [**label classes and offsets for anchor boxes**] (the `anchors` argument) using ground-truth bounding boxes (the `labels` argument).
This function sets the background class to zero and increments the integer index of a new class by one.

**Given:**

anchors: anchor boxes (1 batch only)

labels: ground-truth labels per image in the batch, shape:
(batch_size, num_gt_boxes, 5) — where each GT box is:
[class_label, x_min, y_min, x_max, y_max]

**It returns:**

bbox_offset: encoded offsets between anchors and assigned GT boxes

bbox_mask: mask indicating which anchors are positive (1) or background (0)

class_labels: one label per anchor (0 = background)

"""

def multibox_target(anchors, labels):
    """Label anchor boxes using ground-truth bounding boxes."""

    # Squeeze batch dim from anchors (assumed shape [1, N, 4] → [N, 4])
    # Get number of training images
    batch_size, anchors = labels.shape[0], anchors.squeeze(0)
    batch_offset, batch_mask, batch_class_labels = [], [], []
    device, num_anchors = anchors.device, anchors.shape[0]
    for i in range(batch_size):
        label = labels[i, :, :]

        #Call earlier function to assign each anchor to the closest GT box (returns indices or -1)
        anchors_bbox_map = assign_anchor_to_bbox(
            label[:, 1:], anchors, device)

        # Shape: (num_anchors, 4)
        # Tells which anchors are matched to GT (positive)
        # Used later to zero out offsets for background anchors
        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(
            1, 4)
        # Initialize class labels and assigned bounding box coordinates with
        # zeros
        class_labels = torch.zeros(num_anchors, dtype=torch.long,
                                   device=device)
        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,
                                  device=device)
        # Label classes of anchor boxes using their assigned ground-truth
        # bounding boxes. If an anchor box is not assigned any, we label its
        # class as background (the value remains zero)

        #Indices of anchors that have been matched to a GT box
        indices_true = torch.nonzero(anchors_bbox_map >= 0)
        #For each matched anchor, get the corresponding GT box index
        bb_idx = anchors_bbox_map[indices_true]

        #Assign class labels (incremented by 1, since 0 = background)
        class_labels[indices_true] = label[bb_idx, 0].long() + 1
        #Store the GT box coordinates for the matched anchors
        assigned_bb[indices_true] = label[bb_idx, 1:]
        # Offset transformation
        # Compute center/size difference between anchor and GT
        # Mask out the offsets for background anchors
        offset = offset_boxes(anchors, assigned_bb) * bbox_mask

        #Flatten offsets and masks for easy batch training
        batch_offset.append(offset.reshape(-1))
        batch_mask.append(bbox_mask.reshape(-1))
        batch_class_labels.append(class_labels)

    # bbox_offset: shape (batch_size, num_anchors × 4)
    # bbox_mask: shape (batch_size, num_anchors × 4)
    # class_labels: shape (batch_size, num_anchors)
    # These are ready to be used in a classification + localization loss function (e.g., softmax + smooth L1).
    bbox_offset = torch.stack(batch_offset)
    bbox_mask = torch.stack(batch_mask)
    class_labels = torch.stack(batch_class_labels)
    return (bbox_offset, bbox_mask, class_labels)

"""### An Example

Let's illustrate anchor box labeling
via a concrete example.
We define ground-truth bounding boxes for the dog and cat in the loaded image,
where the first element is the class (0 for dog and 1 for cat) and the remaining four elements are the
$(x, y)$-axis coordinates
at the upper-left corner and the lower-right corner
(range is between 0 and 1).
We also construct five anchor boxes to be labeled
using the coordinates of
the upper-left corner and the lower-right corner:
$A_0, \ldots, A_4$ (the index starts from 0).
Then we [**plot these ground-truth bounding boxes
and anchor boxes
in the image.**]

"""

ground_truth = torch.tensor([[0, 0.1, 0.08, 0.52, 0.92],
                         [1, 0.55, 0.2, 0.9, 0.88]])
anchors = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],
                    [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],
                    [0.57, 0.3, 0.92, 0.9]])

fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k')
show_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4']);

"""Using the `multibox_target` function defined above,
we can [**label classes and offsets
of these anchor boxes based on
the ground-truth bounding boxes**] for the dog and cat.
In this example, indices of
the background, dog, and cat classes
are 0, 1, and 2, respectively.
Below we add an dimension for examples of anchor boxes and ground-truth bounding boxes.

"""

labels = multibox_target(anchors.unsqueeze(dim=0),
                         ground_truth.unsqueeze(dim=0))

"""There are three items in the returned result, all of which are in the tensor format.
The third item contains the labeled classes of the input anchor boxes.

Let's analyze the returned class labels below based on
anchor box and ground-truth bounding box positions in the image.
First, among all the pairs of anchor boxes
and ground-truth bounding boxes,
the IoU of the anchor box $A_4$ and the ground-truth bounding box of the cat is the largest.
Thus, the class of $A_4$ is labeled as the cat.
Taking out
pairs containing $A_4$ or the ground-truth bounding box of the cat, among the rest
the pair of the anchor box $A_1$ and the ground-truth bounding box of the dog has the largest IoU.
So the class of $A_1$ is labeled as the dog.
Next, we need to traverse through the remaining three unlabeled anchor boxes: $A_0$, $A_2$, and $A_3$.
For $A_0$,
the class of the ground-truth bounding box with the largest IoU is the dog,
but the IoU is below the predefined threshold (0.5),
so the class is labeled as background;
for $A_2$,
the class of the ground-truth bounding box with the largest IoU is the cat and the IoU exceeds the threshold, so the class is labeled as the cat;
for $A_3$,
the class of the ground-truth bounding box with the largest IoU is the cat, but the value is below the threshold, so the class is labeled as background.

"""

labels[2]

"""The second returned item is a mask variable of the shape (batch size, four times the number of anchor boxes).
Every four elements in the mask variable
correspond to the four offset values of each anchor box.
Since we do not care about background detection,
offsets of this negative class should not affect the objective function.
Through elementwise multiplications, zeros in the mask variable will filter out negative class offsets before calculating the objective function.

"""

labels[1]

"""The first returned item contains the four offset values labeled for each anchor box.
Note that the offsets of negative-class anchor boxes are labeled as zeros.

"""

labels[0]

"""## Predicting Bounding Boxes with Non-Maximum Suppression


During prediction,
we generate multiple anchor boxes for the image and predict classes and offsets for each of them.
A *predicted bounding box*
is thus obtained according to
an anchor box with its predicted offset.
Below we implement the `offset_inverse` function
that takes in anchors and
offset predictions as inputs and [**applies inverse offset transformations to
return the predicted bounding box coordinates**].

"""

#During training, we encoded the ground-truth box relative to the anchor using a transformation: Offset =
#\left( \frac{x_b - x_a}{w_a},\ \frac{y_b - y_a}{h_a},\ \log \frac{w_b}{w_a},\ \log \frac{h_b}{h_a} \right) ]
#During inference, we invert this transformation:
#Use predicted offsets and the anchor boxes
#Recover predicted bounding box coordinates
#anchors: Tensor of shape (num_anchors, 4), in corner format:
#(𝑥_min,𝑦_min,𝑥_max,𝑦_max)
#offset_preds: Tensor of shape (num_anchors, 4) — predicted offsets from the model
def offset_inverse(anchors, offset_preds):
    """Predict bounding boxes based on anchor boxes with predicted offsets."""
    #Convert anchor boxes to center format
    anc = d2l.box_corner_to_center(anchors)
    #Decode center coordinates
    pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]
    #Decode width and height
    pred_bbox_wh = torch.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]
    #Combine center and size
    pred_bbox = torch.cat((pred_bbox_xy, pred_bbox_wh), axis=1)
    #Convert back to corner format
    ##(x_c,y_c ,w,h)⇒(x_min,y_min ,x_max ,y_max)
    predicted_bbox = d2l.box_center_to_corner(pred_bbox)
    return predicted_bbox

"""When there are many anchor boxes,
many similar (with significant overlap)
predicted bounding boxes
can be potentially output for surrounding the same object.
To simplify the output,
we can merge similar predicted bounding boxes
that belong to the same object
by using *non-maximum suppression* (NMS).

Here is how non-maximum suppression works.
For a predicted bounding box $B$,
the object detection model calculates the predicted likelihood
for each class.
Denoting by $p$ the largest predicted likelihood,
the class corresponding to this probability is the predicted class for $B$.
Specifically, we refer to $p$ as the *confidence* (score) of the predicted bounding box $B$.
On the same image,
all the predicted non-background bounding boxes
are sorted by confidence in descending order
to generate a list $L$.
Then we manipulate the sorted list $L$ in the following steps:

1. Select the predicted bounding box $B_1$ with the highest confidence from $L$ as a basis and remove all non-basis predicted bounding boxes whose IoU with $B_1$ exceeds a predefined threshold $\epsilon$ from $L$. At this point, $L$ keeps the predicted bounding box with the highest confidence but drops others that are too similar to it. In a nutshell, those with *non-maximum* confidence scores are *suppressed*.
1. Select the predicted bounding box $B_2$ with the second highest confidence from $L$ as another basis and remove all non-basis predicted bounding boxes whose IoU with $B_2$ exceeds $\epsilon$ from $L$.
1. Repeat the above process until all the predicted bounding boxes in $L$ have been used as a basis. At this time, the IoU of any pair of predicted bounding boxes in $L$ is below the threshold $\epsilon$; thus, no pair is too similar with each other.
1. Output all the predicted bounding boxes in the list $L$.

[**The following `nms` function sorts confidence scores in descending order and returns their indices.**]

"""

def nms(boxes, scores, iou_threshold):
    """Sort confidence scores of predicted bounding boxes."""
    B = torch.argsort(scores, dim=-1, descending=True)
    keep = []  # Indices of predicted bounding boxes that will be kept
    while B.numel() > 0:
        i = B[0]
        keep.append(i)
        if B.numel() == 1: break
        iou = box_iou(boxes[i, :].reshape(-1, 4),
                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)
        inds = torch.nonzero(iou <= iou_threshold).reshape(-1)
        B = B[inds + 1]
    return torch.tensor(keep, device=boxes.device)

"""We define the following `multibox_detection`
to [**apply non-maximum suppression
to predicting bounding boxes**].
Do not worry if you find the implementation
a bit complicated: we will show how it works
with a concrete example right after the implementation.

"""

def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
                       pos_threshold=0.009999999):
    """Predict bounding boxes using non-maximum suppression."""
    device, batch_size = cls_probs.device, cls_probs.shape[0]
    anchors = anchors.squeeze(0)
    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]
    out = []
    for i in range(batch_size):
        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)
        conf, class_id = torch.max(cls_prob[1:], 0)
        predicted_bb = offset_inverse(anchors, offset_pred)
        keep = nms(predicted_bb, conf, nms_threshold)
        # Find all non-`keep` indices and set the class to background
        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)
        combined = torch.cat((keep, all_idx))
        uniques, counts = combined.unique(return_counts=True)
        non_keep = uniques[counts == 1]
        all_id_sorted = torch.cat((keep, non_keep))
        class_id[non_keep] = -1
        class_id = class_id[all_id_sorted]
        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]
        # Here `pos_threshold` is a threshold for positive (non-background)
        # predictions
        below_min_idx = (conf < pos_threshold)
        class_id[below_min_idx] = -1
        conf[below_min_idx] = 1 - conf[below_min_idx]
        pred_info = torch.cat((class_id.unsqueeze(1),
                               conf.unsqueeze(1),
                               predicted_bb), dim=1)
        out.append(pred_info)
    return torch.stack(out)

"""Now let's [**apply the above implementations
to a concrete example with four anchor boxes**].
For simplicity, we assume that the
predicted offsets are all zeros.
This means that the predicted bounding boxes are anchor boxes.
For each class among the background, dog, and cat,
we also define its predicted likelihood.

"""

anchors = torch.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],
                      [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])
offset_preds = torch.tensor([0] * anchors.numel())
cls_probs = torch.tensor([[0] * 4,  # Predicted background likelihood
                      [0.9, 0.8, 0.7, 0.1],  # Predicted dog likelihood
                      [0.1, 0.2, 0.3, 0.9]])  # Predicted cat likelihood

"""We can [**plot these predicted bounding boxes with their confidence on the image.**]

"""

fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, anchors * bbox_scale,
            ['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])

"""Now we can invoke the `multibox_detection` function
to perform non-maximum suppression,
where the threshold is set to 0.5.
Note that we add
a dimension for examples in the tensor input.

We can see that [**the shape of the returned result**] is
(batch size, number of anchor boxes, 6).
The six elements in the innermost dimension
gives the output information for the same predicted bounding box.
The first element is the predicted class index, which starts from 0 (0 is dog and 1 is cat). The value -1 indicates background or removal in non-maximum suppression.
The second element is the confidence of the predicted bounding box.
The remaining four elements are the $(x, y)$-axis coordinates of the upper-left corner and
the lower-right corner of the predicted bounding box, respectively (range is between 0 and 1).

"""

output = multibox_detection(cls_probs.unsqueeze(dim=0),
                            offset_preds.unsqueeze(dim=0),
                            anchors.unsqueeze(dim=0),
                            nms_threshold=0.5)
output

"""After removing those predicted bounding boxes
of class -1,
we can [**output the final predicted bounding box
kept by non-maximum suppression**].

"""

fig = d2l.plt.imshow(img)
for i in output[0].detach().numpy():
    if i[0] == -1:
        continue
    label = ('dog=', 'cat=')[int(i[0])] + str(i[1])
    show_bboxes(fig.axes, [torch.tensor(i[2:]) * bbox_scale], label)

"""In practice, we can remove predicted bounding boxes with lower confidence even before performing non-maximum suppression, thereby reducing computation in this algorithm.
We may also post-process the output of non-maximum suppression, for example, by only keeping
results with higher confidence
in the final output.


## Summary

* We generate anchor boxes with different shapes centered on each pixel of the image.
* Intersection over union (IoU), also known as Jaccard index, measures the similarity of two bounding boxes. It is the ratio of their intersection area to their union area.
* In a training set, we need two types of labels for each anchor box. One is the class of the object relevant to the anchor box and the other is the offset of the ground-truth bounding box relative to the anchor box.
* During prediction, we can use non-maximum suppression (NMS) to remove similar predicted bounding boxes, thereby simplifying the output.




"""