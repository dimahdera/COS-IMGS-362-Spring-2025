{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GI62_BzMl04o"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import time, sys\n",
        "import pickle\n",
        "import timeit\n",
        "#import wandb\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "from tensorflow.keras import layers, constraints\n",
        "\n",
        "#os.environ[\"WANDB_API_KEY\"] = \"\"\n",
        "plt.ioff()\n",
        "from keras.constraints import Constraint\n",
        "import keras\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update_progress() : Displays or updates a console progress bar\n",
        "## Accepts a float between 0 and 1. Any int will be converted to a float.\n",
        "## A value under 0 represents a 'halt'.\n",
        "## A value at 1 or bigger represents 100%\n",
        "def update_progress(progress):\n",
        "    barLength = 10 # Modify this to change the length of the progress bar\n",
        "    status = \"\"\n",
        "    if isinstance(progress, int):\n",
        "        progress = float(progress)\n",
        "    if not isinstance(progress, float):\n",
        "        progress = 0\n",
        "        status = \"error: progress var must be float\\r\\n\"\n",
        "    if progress < 0:\n",
        "        progress = 0\n",
        "        status = \"Halt...\\r\\n\"\n",
        "    if progress >= 1:\n",
        "        progress = 1\n",
        "        status = \"Done...\\r\\n\"\n",
        "    block = int(round(barLength*progress))\n",
        "    text = \"\\rPercent: [{0}] {1}% {2}\".format( \"#\"*block + \"-\"*(barLength-block), progress*100, status)\n",
        "    sys.stdout.write(text)\n",
        "    sys.stdout.flush()\n",
        "#############################################\n",
        "class D_ReLU(keras.layers.Layer):\n",
        "    \"\"\"ReLU\"\"\"\n",
        "    def __init__(self):\n",
        "        super(D_ReLU, self).__init__()\n",
        "\n",
        "    def call(self, mu_in):\n",
        "        mu_out = tf.nn.relu(mu_in)\n",
        "        return mu_out\n",
        "#############################################\n",
        "class D_GeLU(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(D_GeLU, self).__init__()\n",
        "    def call(self, mu_in):  # mu_in = [50,17,64], Sigma_in= [50,17,64]\n",
        "        mu_out = tf.nn.gelu(mu_in)  # [50,17,64]\n",
        "        return mu_out  # [50,2,17,64], [50,2,17,64,64]\n",
        "#############################################\n",
        "class D_Softmax(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(D_Softmax, self).__init__()\n",
        "    def call(self, mu_in):\n",
        "        mu_dim = tf.cast(tf.shape(mu_in)[-1], tf.float32)\n",
        "        mu_out = tf.nn.softmax(mu_in)\n",
        "        return mu_out\n",
        "\n",
        "#############################################\n",
        "class D_Dropout(keras.layers.Layer):\n",
        "    def __init__(self, drop_prop):\n",
        "        super(D_Dropout, self).__init__()\n",
        "        self.drop_prop = drop_prop\n",
        "\n",
        "    def call(self, mu_in, Training=None):\n",
        "        # shape=[batch_size, seq length, embedding_dim]\n",
        "        if Training:\n",
        "            mu_out = tf.nn.dropout(mu_in, rate=self.drop_prop)  # [50,17,64] or [50, 10]\n",
        "        else:\n",
        "            mu_out = mu_in\n",
        "        return mu_out  # [50,17,64], [50,17,64]\n",
        "#############################################\n",
        "class D_Dense(keras.layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        '''\n",
        "        Initialize the instance attributes\n",
        "        '''\n",
        "        super(D_Dense, self).__init__()\n",
        "        self.units = units\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(name='kernel',\n",
        "                                 shape=(input_shape[-1], self.units),\n",
        "                                 initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),\n",
        "                                 trainable=True)\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) #+ self.b\n",
        "\n",
        "#############################################\n",
        "class D_MLP(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_features, out_features, dropout_rate=0.1):\n",
        "        super(D_MLP, self).__init__()\n",
        "        self.dense1 = D_Dense(hidden_features)\n",
        "        self.dense2 = D_Dense(out_features)\n",
        "        self.dropout1 = D_Dropout(dropout_rate)\n",
        "        self.gelu_1 = D_GeLU()\n",
        "\n",
        "    def call(self, mu_in,training):\n",
        "        mu_out = self.dense1(mu_in)\n",
        "        mu_out = self.gelu_1(mu_out)\n",
        "        mu_out = self.dropout1(mu_out,Training=training)\n",
        "        mu_out = self.dense2(mu_out)\n",
        "        mu_out = self.dropout1(mu_out, Training= training)\n",
        "        return mu_out\n",
        "\n",
        "#############################################\n",
        "class D_LayerNorm(tf.keras.layers.Layer):\n",
        "    def __init__(self, eps=1e-6, **kwargs):\n",
        "        self.eps = eps\n",
        "        super(D_LayerNorm, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(shape=input_shape[-1:],\n",
        "                                     initializer=tf.keras.initializers.Ones(), name='gamma',trainable=True)\n",
        "        self.beta = self.add_weight(shape=input_shape[-1:],\n",
        "                                    initializer=tf.keras.initializers.Zeros(), name='beta', trainable=True)\n",
        "        super(D_LayerNorm, self).build(input_shape)\n",
        "    def call(self, x):\n",
        "        mean = tf.math.reduce_mean(x, axis=-1, keepdims=True)\n",
        "        std = tf.math.reduce_std(x, axis=-1, keepdims=True)\n",
        "        # print( \"mean of LN\",mean.shape)\n",
        "        # print(\"std of LN\",std.shape)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "class D_MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8, dropout = 0.1):\n",
        "        super(D_MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads  # dimensionality of each attention head\n",
        "        # create four dense layers (i.e., fully connected layers) that will be used in the attention mechanism.\n",
        "        self.query_dense = D_Dense(embed_dim)\n",
        "        self.key_dense = D_Dense(embed_dim)\n",
        "        self.value_dense = D_Dense(embed_dim)\n",
        "        self.combine_heads = D_Dense(embed_dim)\n",
        "        self.dropout = D_Dropout(dropout)\n",
        "\n",
        "    def attention(self, query, key, value, training):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        weights = self.dropout(weights, Training = training) #According to original VIT PAPER\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "        '''\n",
        "        This is a helper function that separates the num_heads attention heads\n",
        "        from the input tensor x. It first reshapes x to split the last dimension\n",
        "        into two dimensions of size num_heads and projection_dim, and\n",
        "        then transposes the resulting tensor to move the num_heads dimension to the second position.\n",
        "        '''\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])  # [50,2,17,32]\n",
        "    def call(self, inputs,training):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        '''\n",
        "        These lines apply dense layers to the input tensor inputs to get the key\n",
        "        and value tensors, respectively. The key_dense and value_dense layers are\n",
        "        initialized in the constructor of the class and are fully connected layers\n",
        "        with an output size of embed_dim. The key and value tensors have the same\n",
        "        shape as the inputs tensor except for last dimension, which is embed_dim.\n",
        "           '''\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        '''\n",
        "        These lines reshape the query, key, and value tensors into a tensor with\n",
        "        shape (batch_size, num_heads, seq_len, projection_dim) using the\n",
        "        separate_heads method. This method reshapes the last dimension of the input\n",
        "         tensor into projection_dim and then reshapes the tensor into num_heads\n",
        "         separate tensors along the second dimension.\n",
        "         '''\n",
        "        query = self.separate_heads(query, batch_size)\n",
        "        key = self.separate_heads(key, batch_size)\n",
        "        value = self.separate_heads(value, batch_size)\n",
        "        '''\n",
        "        This line applies the attention method to the query, key, and value tensors\n",
        "        to compute the attention weights and the weighted sum of the value tensors.\n",
        "        The attention method takes the query, key, and value tensors as inputs and\n",
        "        returns the output tensor and the weights tensor.\n",
        "           '''\n",
        "        attention, weights = self.attention(query, key, value, training=training)  # [50,2,17,32]\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  # [50,17,2,32]\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
        "        output = self.combine_heads(concat_attention)\n",
        "        return output\n",
        "#############################################\n",
        "class D_TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
        "        super(D_TransformerBlock, self).__init__()\n",
        "        self.att = D_MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
        "        self.mlp = D_MLP(embed_dim * 2, embed_dim, dropout)\n",
        "        # self.elu_1 = DGeLU()\n",
        "        self.layernorm = D_LayerNorm(eps=1e-6)\n",
        "        #self.dropout = D_Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        inputs_norm = self.layernorm(inputs)\n",
        "        attn_output = self.att(inputs_norm, training=training)\n",
        "        #attn_output = self.dropout(attn_output, Training=training)\n",
        "        out1 = attn_output + inputs\n",
        "        out1_norm = self.layernorm(out1)\n",
        "        mlp_output = self.mlp(out1_norm, training=training)\n",
        "        #mlp_output = self.dropout(mlp_output, Training=training)\n",
        "        return mlp_output + out1"
      ],
      "metadata": {
        "id": "_dQxD4aDqaxd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class D_ViT(tf.keras.Model):\n",
        "    def __init__(\n",
        "            self,\n",
        "            image_size=28,\n",
        "            patch_size=4,\n",
        "            num_layers=7,\n",
        "            num_classes=10,\n",
        "            embed_dim=64,\n",
        "            num_heads=4,\n",
        "            mlp_dim=64,\n",
        "            channels=1,\n",
        "            dropout=0.1,\n",
        "            name=None,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super(D_ViT, self).__init__(**kwargs)\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        self.patch_dim = channels * (patch_size ** 2)\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "        self.num_heads = num_heads\n",
        "        self.channels = channels\n",
        "        self.dropout_rate = dropout\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.pos_emb = self.add_weight(shape=(1, num_patches + 1, embed_dim),name = \"pos_emb\")\n",
        "        self.class_emb = self.add_weight(shape=(1, 1, embed_dim),name = \"class_emb\")\n",
        "        self.patch_proj = D_Dense(embed_dim)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            D_TransformerBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.mlp_head = D_MLP(mlp_dim, num_classes)\n",
        "        self.softmax_out = D_Softmax()\n",
        "        self.dropout = D_Dropout(dropout)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # This ensures the model is built before loading weights\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        \"\"\"Initialize the model by running a forward pass with dummy data.\"\"\"\n",
        "        dummy_input = tf.random.normal((1, self.image_size, self.image_size, self.channels))\n",
        "        _ = self(dummy_input, training=False)\n",
        "\n",
        "    def load_weights_safely(self, weights_path):\n",
        "        \"\"\"Safely load weights after ensuring model is initialized.\"\"\"\n",
        "        # Initialize the model if not already done\n",
        "        if not self.built:\n",
        "            self.initialize_weights()\n",
        "        # Load the weights\n",
        "        self.load_weights(weights_path)\n",
        "\n",
        "    def extract_patches(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\")\n",
        "        patches = tf.reshape(patches, [batch_size, -1, self.patch_dim])\n",
        "        return patches\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        patches = self.extract_patches(x)\n",
        "        x = self.patch_proj(patches)\n",
        "        class_emb = tf.broadcast_to(self.class_emb, [batch_size, 1, self.embed_dim])\n",
        "        x = tf.concat([class_emb, x], axis=1)\n",
        "        x = x + self.pos_emb\n",
        "        x = self.dropout(x, Training=training)  #According to original VIT Paper\n",
        "        for layer in self.enc_layers:\n",
        "            x = layer(x, training = training)\n",
        "        x = self.mlp_head(x[:, 0],training=training)\n",
        "        x = self.softmax_out(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wi81XrXArHVA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_function(image_size=28, patch_size=4, num_layers=5, num_classes=10, embed_dim=32, num_heads=4, mlp_dim=32,\n",
        "                  channels=1, drop_prob=0.01, batch_size=50, epochs=5, lr=0.001, lr_end=0.00001,\n",
        "                  Targeted=False, Random_noise=False, gaussain_noise_std=0.3, Training=True, Testing=False,\n",
        "                  ):\n",
        "\n",
        "    PATH = './trans_epoch_{}/'.format(epochs)\n",
        "    mnist = tf.keras.datasets.mnist\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    x_train = tf.expand_dims(x_train, -1)\n",
        "    x_test = tf.expand_dims(x_test, -1)\n",
        "    one_hot_y_train = tf.one_hot(y_train.astype(np.float32), depth=num_classes)\n",
        "    one_hot_y_test = tf.one_hot(y_test.astype(np.float32), depth=num_classes)\n",
        "    tr_dataset = tf.data.Dataset.from_tensor_slices((x_train, one_hot_y_train)).batch(batch_size)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, one_hot_y_test)).batch(batch_size)\n",
        "\n",
        "    trans_model = D_ViT(image_size=image_size, patch_size=patch_size, num_layers=num_layers, num_classes=num_classes,\n",
        "                          embed_dim=embed_dim, num_heads=num_heads, mlp_dim=mlp_dim,\n",
        "                          channels=channels, dropout=drop_prob, name='det_trans')\n",
        "\n",
        "    num_train_steps = epochs * int(x_train.shape[0] / batch_size)\n",
        "    #    step = min(step, decay_steps)\n",
        "    #    ((initial_learning_rate - end_learning_rate) * (1 - step / decay_steps) ^ (power) ) + end_learning_rate\n",
        "    learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=lr,\n",
        "                                                                     decay_steps=num_train_steps,\n",
        "                                                                     end_learning_rate=lr_end, power=1.)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn , clipvalue=1.0)\n",
        "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "    @tf.function  # Make it fast.\n",
        "    def train_on_batch(x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            out = trans_model(x, training=True)\n",
        "            trans_model.trainable = True\n",
        "            loss = loss_fn(y, out)\n",
        "\n",
        "        gradients = tape.gradient(loss, trans_model.trainable_weights)\n",
        "        gradients = [(tf.where(tf.math.is_nan(grad), tf.constant(1.0e-5, shape=grad.shape), grad)) for grad in gradients]\n",
        "        gradients = [(tf.where(tf.math.is_inf(grad), tf.constant(1.0e-5, shape=grad.shape), grad)) for grad in gradients]\n",
        "        optimizer.apply_gradients(zip(gradients, trans_model.trainable_weights))\n",
        "        return loss, out, gradients\n",
        "    @tf.function\n",
        "    def validation_on_batch(x, y):\n",
        "        trans_model.trainable = False\n",
        "        out = trans_model(x, training=False)\n",
        "        vloss = loss_fn(y, out)\n",
        "\n",
        "        return vloss, out\n",
        "\n",
        "    @tf.function\n",
        "    def test_on_batch(x, y):\n",
        "        trans_model.trainable = False\n",
        "        out = trans_model(x, training=False)\n",
        "        return out\n",
        "\n",
        "    if Training:\n",
        "        trans_model.initialize_weights()\n",
        "        if os.path.exists(PATH):\n",
        "            shutil.rmtree(PATH)\n",
        "        os.makedirs(PATH)\n",
        "        # wandb.init(entity=\"transformer\",\n",
        "        #            project=\"Trans_mnist_epochs_{}_layer_{}_lr_{}_batch_size_{}_dimension_{}_patch_size_{}_head_{}_drop_{}\".format(\n",
        "        #                epochs, num_layers, lr, batch_size, embed_dim, patch_size, num_heads,drop_prob))\n",
        "\n",
        "        train_acc = np.zeros(epochs)\n",
        "        valid_acc = np.zeros(epochs)\n",
        "        train_err = np.zeros(epochs)\n",
        "        valid_error = np.zeros(epochs)\n",
        "        start = timeit.default_timer()\n",
        "        for epoch in range(epochs):\n",
        "            print('Epoch: ', epoch+1, '/' , epochs)\n",
        "            acc1 = 0\n",
        "            acc_valid1 = 0\n",
        "            err1 = 0\n",
        "            err_valid1 = 0\n",
        "            tr_no_steps = 0\n",
        "            va_no_steps = 0\n",
        "            #-------------Training--------------------\n",
        "            acc_training = np.zeros(int(x_train.shape[0] / (batch_size)))\n",
        "            err_training = np.zeros(int(x_train.shape[0] / (batch_size)))\n",
        "            for step, (x, y) in enumerate(tr_dataset):\n",
        "                update_progress(step/int(x_train.shape[0]/(batch_size)) )\n",
        "                loss, mu_out, gradients = train_on_batch(x, y)\n",
        "                err1+= loss.numpy()\n",
        "                corr = tf.equal(tf.math.argmax(mu_out, axis=1),tf.math.argmax(y,axis=1))\n",
        "                accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n",
        "                acc1+=accuracy.numpy()\n",
        "                if step % 100 == 0:\n",
        "                    print('\\n gradient', np.mean(gradients[0].numpy()))\n",
        "                    print(\"\\n Step:\", step, \"Loss:\" , float(err1/(tr_no_steps + 1.)))\n",
        "                    print(\"Total Training accuracy so far: %.3f\" % float(acc1/(tr_no_steps + 1.)))\n",
        "                tr_no_steps+=1\n",
        "                # wandb.log({\"Total Training Loss\": loss.numpy() ,\n",
        "                #             \"Training Accuracy per minibatch\": accuracy.numpy() ,\n",
        "                #             \"gradient per minibatch\": np.mean(gradients[0]),\n",
        "                #             'epoch': epoch\n",
        "                #    })\n",
        "            train_acc[epoch] = acc1/tr_no_steps\n",
        "            train_err[epoch] = err1/tr_no_steps\n",
        "            print('Training Acc  ', train_acc[epoch])\n",
        "            print('Training error  ', train_err[epoch])\n",
        "            #---------------Validation----------------------\n",
        "            for step, (x, y) in enumerate(val_dataset):\n",
        "                update_progress(step / int(x_test.shape[0] / (batch_size)) )\n",
        "                total_vloss, mu_out = validation_on_batch(x, y)\n",
        "                err_valid1+= total_vloss.numpy()\n",
        "                corr = tf.equal(tf.math.argmax(mu_out, axis=-1),tf.math.argmax(y,axis=-1))\n",
        "                va_accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n",
        "                acc_valid1+=va_accuracy.numpy()\n",
        "\n",
        "                if step % 100 == 0:\n",
        "                    print(\"Step:\", step, \"Loss:\", float(total_vloss))\n",
        "                    print(\"Total validation accuracy so far: %.3f\" % va_accuracy)\n",
        "                va_no_steps+=1\n",
        "                # wandb.log({\"Total Validation Loss\": total_vloss.numpy() ,\n",
        "                #               \"Validation Acuracy per minibatch\": va_accuracy.numpy()\n",
        "                #                })\n",
        "            valid_acc[epoch] = acc_valid1/va_no_steps\n",
        "            valid_error[epoch] = err_valid1/va_no_steps\n",
        "            stop = timeit.default_timer()\n",
        "            if np.max(valid_acc) == valid_acc[epoch]:\n",
        "                trans_model.save_weights(PATH+'Trans_model_best.weights.h5')\n",
        "                #trans_model.save(PATH + 'Deterministic_cnn_model_best.keras')\n",
        "\n",
        "            #trans_model.save_weights(PATH + 'vdp_trans_model')\n",
        "            # wandb.log({\"Average Training Loss\":  train_err[epoch],\n",
        "            #            \"Average Training Accuracy\": train_acc[epoch],\n",
        "            #            \"Average Validation Loss\": valid_error[epoch],\n",
        "            #            \"Average Validation Accuracy\": valid_acc[epoch],\n",
        "            #            'epoch': epoch\n",
        "            #           })\n",
        "            print('Total Training Time: ', stop - start)\n",
        "            print('Training Acc  ', train_acc[epoch])\n",
        "            print('Validation Acc  ', valid_acc[epoch])\n",
        "            print('------------------------------------')\n",
        "            print('Training error  ', train_err[epoch])\n",
        "            print('Validation error  ', valid_error[epoch])\n",
        "        #-----------------End Training--------------------------\n",
        "        trans_model.save_weights(PATH+'Trans_model_last.weights.h5')\n",
        "        if (epochs > 1):\n",
        "            fig = plt.figure(figsize=(15,7))\n",
        "            plt.plot(train_acc, 'b', label='Training acc')\n",
        "            plt.plot(valid_acc,'r' , label='Validation acc')\n",
        "            plt.ylim(0, 1.1)\n",
        "            plt.title(\"Transformer on MNIST Data\")\n",
        "            plt.xlabel(\"Epochs\")\n",
        "            plt.ylabel(\"Accuracy\")\n",
        "            plt.legend(loc='lower right')\n",
        "            plt.savefig(PATH + 'Trans_on_MNIST_Data_acc.png')\n",
        "            plt.close(fig)\n",
        "\n",
        "            fig = plt.figure(figsize=(15,7))\n",
        "            plt.plot(train_err, 'b', label='Training error')\n",
        "            plt.plot(valid_error,'r' , label='Validation error')\n",
        "            plt.title(\"Transformer on MNIST Data\")\n",
        "            plt.xlabel(\"Epochs\")\n",
        "            plt.ylabel(\"Error\")\n",
        "            plt.legend(loc='upper right')\n",
        "            plt.savefig(PATH + 'Trans_on_MNIST_Data_error.png')\n",
        "            plt.close(fig)\n",
        "\n",
        "        f = open(PATH + 'training_validation_acc_error.pkl', 'wb')\n",
        "        pickle.dump([train_acc, valid_acc, train_err, valid_error], f)\n",
        "        f.close()\n",
        "\n",
        "        textfile = open(PATH + 'Related_hyperparameters.txt','w')\n",
        "        textfile.write(' Input Dimension : ' + str(image_size))\n",
        "        textfile.write('\\n Hidden units : ' + str(mlp_dim))\n",
        "        textfile.write('\\n Number of Classes : ' + str(num_classes))\n",
        "        textfile.write('\\n No of epochs : ' + str(epochs))\n",
        "        textfile.write('\\n Initial Learning rate : ' + str(lr))\n",
        "        textfile.write('\\n Ending Learning rate : ' + str(lr_end))\n",
        "        textfile.write('\\n batch size : ' + str(batch_size))\n",
        "        textfile.write(\"\\n---------------------------------\")\n",
        "        if Training:\n",
        "            textfile.write('\\n Total run time in sec : ' + str(stop - start))\n",
        "            if (epochs == 1):\n",
        "                textfile.write(\"\\n Averaged Training  Accuracy : \" + str(train_acc))\n",
        "                textfile.write(\"\\n Averaged Validation Accuracy : \" + str(valid_acc))\n",
        "\n",
        "                textfile.write(\"\\n Averaged Training  error : \" + str(train_err))\n",
        "                textfile.write(\"\\n Averaged Validation error : \" + str(valid_error))\n",
        "            else:\n",
        "                textfile.write(\"\\n Averaged Training  Accuracy : \" + str(np.mean(train_acc[epoch])))\n",
        "                textfile.write(\"\\n Averaged Validation Accuracy : \" + str(np.mean(valid_acc[epoch])))\n",
        "\n",
        "                textfile.write(\"\\n Averaged Training  error : \" + str(np.mean(train_err[epoch])))\n",
        "                textfile.write(\"\\n Averaged Validation error : \" + str(np.mean(valid_error[epoch])))\n",
        "        textfile.write(\"\\n---------------------------------\")\n",
        "        textfile.write(\"\\n--------------------------------\")\n",
        "        textfile.close()\n",
        "\n",
        "    #-------------------------Testing-----------------------------\n",
        "    if Testing:\n",
        "        test_path = 'test_results/'\n",
        "        if Random_noise:\n",
        "            print(f'Random Noise: {gaussain_noise_std}')\n",
        "            test_path = 'test_results_random_noise_{}/'.format(gaussain_noise_std)\n",
        "        full_test_path = PATH + test_path\n",
        "        if os.path.exists(full_test_path):\n",
        "            # Remove the existing test path and its contents\n",
        "            shutil.rmtree(full_test_path)\n",
        "        os.makedirs(PATH + test_path)\n",
        "        trans_model.load_weights_safely(PATH + 'Trans_model_best.weights.h5')\n",
        "\n",
        "        test_no_steps = 0\n",
        "        true_x = np.zeros([int(x_test.shape[0] / (batch_size)), batch_size, image_size, image_size, 1])\n",
        "        true_y = np.zeros([int(x_test.shape[0] / (batch_size)), batch_size, num_classes])\n",
        "        mu_out_ = np.zeros([int(x_test.shape[0] / (batch_size)), batch_size, num_classes])\n",
        "        acc_test = np.zeros(int(x_test.shape[0] / (batch_size)))\n",
        "        for step, (x, y) in enumerate(val_dataset):\n",
        "            update_progress(step / int(x_test.shape[0] / (batch_size)))\n",
        "            true_x[test_no_steps, :, :, :, :] = x\n",
        "            true_y[test_no_steps, :, :] = y\n",
        "            if Random_noise:\n",
        "                noise = tf.random.normal(shape=[batch_size, image_size, image_size, 1], mean=0.0,\n",
        "                                         stddev=gaussain_noise_std, dtype=x.dtype)\n",
        "                x = x + noise\n",
        "            mu_out = test_on_batch(x, y)\n",
        "            mu_out_[test_no_steps, :, :] = mu_out\n",
        "            corr = tf.equal(tf.math.argmax(mu_out, axis=1), tf.math.argmax(y, axis=1))\n",
        "            accuracy = tf.reduce_mean(tf.cast(corr, tf.float32))\n",
        "            acc_test[test_no_steps] = accuracy.numpy()\n",
        "            if step % 100 == 0:\n",
        "                print(\"Total running accuracy so far: %.3f\" % acc_test[test_no_steps])\n",
        "            test_no_steps += 1\n",
        "\n",
        "        test_acc = np.mean(acc_test)\n",
        "        print('\\nTest accuracy : ', test_acc)\n",
        "\n",
        "\n",
        "        pf = open(PATH + test_path + 'info.pkl', 'wb')\n",
        "        pickle.dump([mu_out_, true_x, true_y, test_acc], pf)\n",
        "        pf.close()\n",
        "\n",
        "\n",
        "        var = np.zeros([int(x_test.shape[0] / (batch_size)), batch_size])\n",
        "        if Random_noise:\n",
        "            snr_signal = np.zeros([int(x_test.shape[0] / (batch_size)), batch_size])\n",
        "            for i in range(int(x_test.shape[0] / (batch_size))):\n",
        "                for j in range(batch_size):\n",
        "                    noise = tf.random.normal(shape=[image_size, image_size, 1], mean=0.0, stddev=gaussain_noise_std,\n",
        "                                             dtype=x.dtype)\n",
        "                    snr_signal[i, j] = 10 * np.log10(np.sum(np.square(true_x[i, j, :, :, :])) / np.sum(np.square(noise)))\n",
        "\n",
        "            print('SNR', np.mean(snr_signal))\n",
        "\n",
        "        textfile = open(PATH + test_path + 'Related_hyperparameters.txt', 'w')\n",
        "        textfile.write(' Input Dimension : ' + str(image_size))\n",
        "        textfile.write('\\n Number of Classes : ' + str(num_classes))\n",
        "        textfile.write('\\n No of epochs : ' + str(epochs))\n",
        "        textfile.write('\\n Initial Learning rate : ' + str(lr))\n",
        "        textfile.write('\\n Ending Learning rate : ' + str(lr_end))\n",
        "        textfile.write('\\n batch size : ' + str(batch_size))\n",
        "        textfile.write(\"\\n---------------------------------\")\n",
        "        textfile.write(\"\\n Test Accuracy : \" + str(test_acc))\n",
        "        textfile.write(\"\\n---------------------------------\")\n",
        "        if Random_noise:\n",
        "            textfile.write('\\n Random Noise std: ' + str(gaussain_noise_std))\n",
        "            textfile.write(\"\\n SNR: \" + str(np.mean(snr_signal)))\n",
        "        textfile.write(\"\\n---------------------------------\")\n",
        "        textfile.close()\n"
      ],
      "metadata": {
        "id": "Dszh4IvvrVd-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "     main_function()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQstm-pUuOfe",
        "outputId": "096c84ab-89e4-4f93-bec1-25fd3e2be29a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1 / 5\n",
            "Percent: [----------] 0.0% \n",
            " gradient 3.6862536e-06\n",
            "\n",
            " Step: 0 Loss: 2.3023521900177\n",
            "Total Training accuracy so far: 0.160\n",
            "Percent: [----------] 4.166666666666666% \n",
            " gradient -2.3734226e-05\n",
            "\n",
            " Step: 50 Loss: 2.211318254470825\n",
            "Total Training accuracy so far: 0.170\n",
            "Percent: [#---------] 8.333333333333332% \n",
            " gradient -3.3725955e-05\n",
            "\n",
            " Step: 100 Loss: 2.0832881927490234\n",
            "Total Training accuracy so far: 0.214\n",
            "Percent: [#---------] 12.5% \n",
            " gradient 6.6263788e-06\n",
            "\n",
            " Step: 150 Loss: 2.0214943885803223\n",
            "Total Training accuracy so far: 0.234\n",
            "Percent: [##--------] 16.666666666666664% \n",
            " gradient 2.3134662e-05\n",
            "\n",
            " Step: 200 Loss: 1.9800227880477905\n",
            "Total Training accuracy so far: 0.250\n",
            "Percent: [##--------] 20.833333333333336% \n",
            " gradient 5.496631e-05\n",
            "\n",
            " Step: 250 Loss: 1.9393558502197266\n",
            "Total Training accuracy so far: 0.263\n",
            "Percent: [##--------] 25.0% \n",
            " gradient 1.3048919e-05\n",
            "\n",
            " Step: 300 Loss: 1.8924305438995361\n",
            "Total Training accuracy so far: 0.280\n",
            "Percent: [###-------] 29.166666666666668% \n",
            " gradient 3.391826e-05\n",
            "\n",
            " Step: 350 Loss: 1.8519548177719116\n",
            "Total Training accuracy so far: 0.292\n",
            "Percent: [###-------] 33.33333333333333% \n",
            " gradient 6.793498e-05\n",
            "\n",
            " Step: 400 Loss: 1.810557246208191\n",
            "Total Training accuracy so far: 0.305\n",
            "Percent: [####------] 37.5% \n",
            " gradient 2.6720232e-05\n",
            "\n",
            " Step: 450 Loss: 1.7739967107772827\n",
            "Total Training accuracy so far: 0.317\n",
            "Percent: [####------] 41.66666666666667% \n",
            " gradient 0.000116102674\n",
            "\n",
            " Step: 500 Loss: 1.7376630306243896\n",
            "Total Training accuracy so far: 0.329\n",
            "Percent: [#####-----] 45.83333333333333% \n",
            " gradient 4.9443646e-05\n",
            "\n",
            " Step: 550 Loss: 1.7047739028930664\n",
            "Total Training accuracy so far: 0.343\n",
            "Percent: [#####-----] 50.0% \n",
            " gradient -0.00010255366\n",
            "\n",
            " Step: 600 Loss: 1.6710419654846191\n",
            "Total Training accuracy so far: 0.357\n",
            "Percent: [#####-----] 54.166666666666664% \n",
            " gradient -0.00022637843\n",
            "\n",
            " Step: 650 Loss: 1.64340078830719\n",
            "Total Training accuracy so far: 0.369\n",
            "Percent: [######----] 58.333333333333336% \n",
            " gradient -1.609474e-05\n",
            "\n",
            " Step: 700 Loss: 1.6096165180206299\n",
            "Total Training accuracy so far: 0.383\n",
            "Percent: [######----] 62.5% \n",
            " gradient -6.71952e-05\n",
            "\n",
            " Step: 750 Loss: 1.5797746181488037\n",
            "Total Training accuracy so far: 0.394\n",
            "Percent: [#######---] 66.66666666666666% \n",
            " gradient 5.7022167e-05\n",
            "\n",
            " Step: 800 Loss: 1.5527020692825317\n",
            "Total Training accuracy so far: 0.406\n",
            "Percent: [#######---] 70.83333333333334% \n",
            " gradient -3.5810805e-05\n",
            "\n",
            " Step: 850 Loss: 1.5256963968276978\n",
            "Total Training accuracy so far: 0.417\n",
            "Percent: [########--] 75.0% \n",
            " gradient 0.00017786537\n",
            "\n",
            " Step: 900 Loss: 1.5016735792160034\n",
            "Total Training accuracy so far: 0.427\n",
            "Percent: [########--] 79.16666666666666% \n",
            " gradient -3.5536188e-05\n",
            "\n",
            " Step: 950 Loss: 1.4789669513702393\n",
            "Total Training accuracy so far: 0.436\n",
            "Percent: [########--] 83.33333333333334% \n",
            " gradient 8.860466e-05\n",
            "\n",
            " Step: 1000 Loss: 1.454561471939087\n",
            "Total Training accuracy so far: 0.446\n",
            "Percent: [#########-] 87.5% \n",
            " gradient -0.000105272775\n",
            "\n",
            " Step: 1050 Loss: 1.4327999353408813\n",
            "Total Training accuracy so far: 0.455\n",
            "Percent: [#########-] 91.66666666666666% \n",
            " gradient 9.015895e-05\n",
            "\n",
            " Step: 1100 Loss: 1.409232258796692\n",
            "Total Training accuracy so far: 0.465\n",
            "Percent: [##########] 95.83333333333334% \n",
            " gradient -1.1142236e-05\n",
            "\n",
            " Step: 1150 Loss: 1.3860818147659302\n",
            "Total Training accuracy so far: 0.474\n",
            "Percent: [##########] 99.91666666666667% Training Acc   0.4814661741256714\n",
            "Training error   1.3655320405960083\n",
            "Percent: [----------] 0.0% Step: 0 Loss: 0.7106568813323975\n",
            "Total validation accuracy so far: 0.640\n",
            "Percent: [##--------] 25.0% Step: 50 Loss: 1.0528442859649658\n",
            "Total validation accuracy so far: 0.600\n",
            "Percent: [#####-----] 50.0% Step: 100 Loss: 0.5468379855155945\n",
            "Total validation accuracy so far: 0.720\n",
            "Percent: [########--] 75.0% Step: 150 Loss: 0.812274694442749\n",
            "Total validation accuracy so far: 0.680\n",
            "Percent: [##########] 99.5% Total Training Time:  189.7775816819999\n",
            "Training Acc   0.4814661741256714\n",
            "Validation Acc   0.6848002076148987\n",
            "------------------------------------\n",
            "Training error   1.3655320405960083\n",
            "Validation error   0.8096185922622681\n",
            "Epoch:  2 / 5\n",
            "Percent: [----------] 0.0% \n",
            " gradient 6.590807e-07\n",
            "\n",
            " Step: 0 Loss: 1.1192418336868286\n",
            "Total Training accuracy so far: 0.540\n",
            "Percent: [----------] 4.166666666666666% \n",
            " gradient -6.612674e-05\n",
            "\n",
            " Step: 50 Loss: 0.8334667086601257\n",
            "Total Training accuracy so far: 0.709\n",
            "Percent: [#---------] 8.333333333333332% \n",
            " gradient -7.991288e-06\n",
            "\n",
            " Step: 100 Loss: 0.8109574317932129\n",
            "Total Training accuracy so far: 0.720\n",
            "Percent: [#---------] 12.5% \n",
            " gradient 2.662218e-06\n",
            "\n",
            " Step: 150 Loss: 0.7991575598716736\n",
            "Total Training accuracy so far: 0.725\n",
            "Percent: [##--------] 16.666666666666664% \n",
            " gradient 3.0106856e-05\n",
            "\n",
            " Step: 200 Loss: 0.7920346260070801\n",
            "Total Training accuracy so far: 0.732\n",
            "Percent: [##--------] 20.833333333333336% \n",
            " gradient 8.393361e-05\n",
            "\n",
            " Step: 250 Loss: 0.7756038904190063\n",
            "Total Training accuracy so far: 0.738\n",
            "Percent: [##--------] 25.0% \n",
            " gradient -4.327216e-05\n",
            "\n",
            " Step: 300 Loss: 0.769784688949585\n",
            "Total Training accuracy so far: 0.741\n",
            "Percent: [###-------] 29.166666666666668% \n",
            " gradient -0.00023992933\n",
            "\n",
            " Step: 350 Loss: 0.7587828040122986\n",
            "Total Training accuracy so far: 0.747\n",
            "Percent: [###-------] 33.33333333333333% \n",
            " gradient 3.2001903e-05\n",
            "\n",
            " Step: 400 Loss: 0.7465163469314575\n",
            "Total Training accuracy so far: 0.752\n",
            "Percent: [####------] 37.5% \n",
            " gradient -0.00011909362\n",
            "\n",
            " Step: 450 Loss: 0.7326071858406067\n",
            "Total Training accuracy so far: 0.758\n",
            "Percent: [####------] 41.66666666666667% \n",
            " gradient -7.2625524e-05\n",
            "\n",
            " Step: 500 Loss: 0.7248082756996155\n",
            "Total Training accuracy so far: 0.761\n",
            "Percent: [#####-----] 45.83333333333333% \n",
            " gradient 0.00013021709\n",
            "\n",
            " Step: 550 Loss: 0.7110979557037354\n",
            "Total Training accuracy so far: 0.767\n",
            "Percent: [#####-----] 50.0% \n",
            " gradient -0.00017179364\n",
            "\n",
            " Step: 600 Loss: 0.7027398943901062\n",
            "Total Training accuracy so far: 0.769\n",
            "Percent: [#####-----] 54.166666666666664% \n",
            " gradient 6.968627e-05\n",
            "\n",
            " Step: 650 Loss: 0.6981938481330872\n",
            "Total Training accuracy so far: 0.772\n",
            "Percent: [######----] 58.333333333333336% \n",
            " gradient -6.0805603e-05\n",
            "\n",
            " Step: 700 Loss: 0.6860329508781433\n",
            "Total Training accuracy so far: 0.776\n",
            "Percent: [######----] 62.5% \n",
            " gradient -0.0001572648\n",
            "\n",
            " Step: 750 Loss: 0.6768571138381958\n",
            "Total Training accuracy so far: 0.779\n",
            "Percent: [#######---] 66.66666666666666% \n",
            " gradient -0.00022323061\n",
            "\n",
            " Step: 800 Loss: 0.6696017384529114\n",
            "Total Training accuracy so far: 0.782\n",
            "Percent: [#######---] 70.83333333333334% \n",
            " gradient -8.2271144e-05\n",
            "\n",
            " Step: 850 Loss: 0.661773145198822\n",
            "Total Training accuracy so far: 0.784\n",
            "Percent: [########--] 75.0% \n",
            " gradient -0.00013487368\n",
            "\n",
            " Step: 900 Loss: 0.6553391218185425\n",
            "Total Training accuracy so far: 0.787\n",
            "Percent: [########--] 79.16666666666666% \n",
            " gradient -5.530902e-05\n",
            "\n",
            " Step: 950 Loss: 0.6507152915000916\n",
            "Total Training accuracy so far: 0.789\n",
            "Percent: [########--] 83.33333333333334% \n",
            " gradient -4.323502e-05\n",
            "\n",
            " Step: 1000 Loss: 0.6463849544525146\n",
            "Total Training accuracy so far: 0.790\n",
            "Percent: [#########-] 87.5% \n",
            " gradient 0.00018552585\n",
            "\n",
            " Step: 1050 Loss: 0.6404415369033813\n",
            "Total Training accuracy so far: 0.792\n",
            "Percent: [#########-] 91.66666666666666% \n",
            " gradient -0.00012959116\n",
            "\n",
            " Step: 1100 Loss: 0.633388340473175\n",
            "Total Training accuracy so far: 0.795\n",
            "Percent: [##########] 95.83333333333334% \n",
            " gradient -0.00010177616\n",
            "\n",
            " Step: 1150 Loss: 0.6261109113693237\n",
            "Total Training accuracy so far: 0.797\n",
            "Percent: [##########] 99.91666666666667% Training Acc   0.8004682064056396\n",
            "Training error   0.617317259311676\n",
            "Percent: [----------] 0.0% Step: 0 Loss: 0.25879523158073425\n",
            "Total validation accuracy so far: 0.940\n",
            "Percent: [##--------] 25.0% Step: 50 Loss: 0.6501567959785461\n",
            "Total validation accuracy so far: 0.760\n",
            "Percent: [#####-----] 50.0% Step: 100 Loss: 0.15311641991138458\n",
            "Total validation accuracy so far: 0.920\n",
            "Percent: [########--] 75.0% Step: 150 Loss: 0.20388223230838776\n",
            "Total validation accuracy so far: 0.940\n",
            "Percent: [##########] 99.5% Total Training Time:  381.95196929799977\n",
            "Training Acc   0.8004682064056396\n",
            "Validation Acc   0.8899999856948853\n",
            "------------------------------------\n",
            "Training error   0.617317259311676\n",
            "Validation error   0.3663794696331024\n",
            "Epoch:  3 / 5\n",
            "Percent: [----------] 0.0% \n",
            " gradient 7.8974816e-05\n",
            "\n",
            " Step: 0 Loss: 0.6488480567932129\n",
            "Total Training accuracy so far: 0.820\n",
            "Percent: [----------] 4.166666666666666% \n",
            " gradient -2.4464354e-05\n",
            "\n",
            " Step: 50 Loss: 0.48337632417678833\n",
            "Total Training accuracy so far: 0.844\n",
            "Percent: [#---------] 8.333333333333332% \n",
            " gradient 0.0001343807\n",
            "\n",
            " Step: 100 Loss: 0.4552585482597351\n",
            "Total Training accuracy so far: 0.849\n",
            "Percent: [#---------] 12.5% \n",
            " gradient 4.007584e-05\n",
            "\n",
            " Step: 150 Loss: 0.46684011816978455\n",
            "Total Training accuracy so far: 0.846\n",
            "Percent: [##--------] 16.666666666666664% \n",
            " gradient -4.860479e-05\n",
            "\n",
            " Step: 200 Loss: 0.4691909849643707\n",
            "Total Training accuracy so far: 0.848\n",
            "Percent: [##--------] 20.833333333333336% \n",
            " gradient -1.2410325e-05\n",
            "\n",
            " Step: 250 Loss: 0.4674227833747864\n",
            "Total Training accuracy so far: 0.849\n",
            "Percent: [##--------] 25.0% \n",
            " gradient -3.223214e-05\n",
            "\n",
            " Step: 300 Loss: 0.4729155898094177\n",
            "Total Training accuracy so far: 0.846\n",
            "Percent: [###-------] 29.166666666666668% \n",
            " gradient 0.00014471357\n",
            "\n",
            " Step: 350 Loss: 0.4709436893463135\n",
            "Total Training accuracy so far: 0.847\n",
            "Percent: [###-------] 33.33333333333333% \n",
            " gradient 5.987726e-05\n",
            "\n",
            " Step: 400 Loss: 0.46522095799446106\n",
            "Total Training accuracy so far: 0.851\n",
            "Percent: [####------] 37.5% \n",
            " gradient -2.8190687e-05\n",
            "\n",
            " Step: 450 Loss: 0.45762136578559875\n",
            "Total Training accuracy so far: 0.853\n",
            "Percent: [####------] 41.66666666666667% \n",
            " gradient -0.00010815288\n",
            "\n",
            " Step: 500 Loss: 0.45413899421691895\n",
            "Total Training accuracy so far: 0.854\n",
            "Percent: [#####-----] 45.83333333333333% \n",
            " gradient 4.5452427e-05\n",
            "\n",
            " Step: 550 Loss: 0.45045745372772217\n",
            "Total Training accuracy so far: 0.855\n",
            "Percent: [#####-----] 50.0% \n",
            " gradient 3.8706094e-05\n",
            "\n",
            " Step: 600 Loss: 0.44851624965667725\n",
            "Total Training accuracy so far: 0.855\n",
            "Percent: [#####-----] 54.166666666666664% \n",
            " gradient 6.765079e-05\n",
            "\n",
            " Step: 650 Loss: 0.4510802626609802\n",
            "Total Training accuracy so far: 0.854\n",
            "Percent: [######----] 58.333333333333336% \n",
            " gradient 8.365689e-05\n",
            "\n",
            " Step: 700 Loss: 0.44482752680778503\n",
            "Total Training accuracy so far: 0.856\n",
            "Percent: [######----] 62.5% \n",
            " gradient -4.393167e-05\n",
            "\n",
            " Step: 750 Loss: 0.44259005784988403\n",
            "Total Training accuracy so far: 0.857\n",
            "Percent: [#######---] 66.66666666666666% \n",
            " gradient -0.00013194008\n",
            "\n",
            " Step: 800 Loss: 0.44169706106185913\n",
            "Total Training accuracy so far: 0.857\n",
            "Percent: [#######---] 70.83333333333334% \n",
            " gradient 5.698653e-05\n",
            "\n",
            " Step: 850 Loss: 0.4394238591194153\n",
            "Total Training accuracy so far: 0.858\n",
            "Percent: [########--] 75.0% \n",
            " gradient -1.9347071e-05\n",
            "\n",
            " Step: 900 Loss: 0.4375731348991394\n",
            "Total Training accuracy so far: 0.859\n",
            "Percent: [########--] 79.16666666666666% \n",
            " gradient -1.7739372e-05\n",
            "\n",
            " Step: 950 Loss: 0.4367232620716095\n",
            "Total Training accuracy so far: 0.859\n",
            "Percent: [########--] 83.33333333333334% \n",
            " gradient 2.6577995e-05\n",
            "\n",
            " Step: 1000 Loss: 0.43564239144325256\n",
            "Total Training accuracy so far: 0.860\n",
            "Percent: [#########-] 87.5% \n",
            " gradient 0.00016569458\n",
            "\n",
            " Step: 1050 Loss: 0.43412619829177856\n",
            "Total Training accuracy so far: 0.860\n",
            "Percent: [#########-] 91.66666666666666% \n",
            " gradient 1.2023463e-05\n",
            "\n",
            " Step: 1100 Loss: 0.4311541020870209\n",
            "Total Training accuracy so far: 0.861\n",
            "Percent: [##########] 95.83333333333334% \n",
            " gradient 0.00014956115\n",
            "\n",
            " Step: 1150 Loss: 0.42758527398109436\n",
            "Total Training accuracy so far: 0.862\n",
            "Percent: [##########] 99.91666666666667% Training Acc   0.8630692362785339\n",
            "Training error   0.4230071008205414\n",
            "Percent: [----------] 0.0% Step: 0 Loss: 0.22692252695560455\n",
            "Total validation accuracy so far: 0.920\n",
            "Percent: [##--------] 25.0% Step: 50 Loss: 0.2919202148914337\n",
            "Total validation accuracy so far: 0.860\n",
            "Percent: [#####-----] 50.0% Step: 100 Loss: 0.04385906830430031\n",
            "Total validation accuracy so far: 1.000\n",
            "Percent: [########--] 75.0% Step: 150 Loss: 0.16578952968120575\n",
            "Total validation accuracy so far: 0.920\n",
            "Percent: [##########] 99.5% Total Training Time:  565.0511695549999\n",
            "Training Acc   0.8630692362785339\n",
            "Validation Acc   0.9214998483657837\n",
            "------------------------------------\n",
            "Training error   0.4230071008205414\n",
            "Validation error   0.25926464796066284\n",
            "Epoch:  4 / 5\n",
            "Percent: [----------] 0.0% \n",
            " gradient 9.25582e-05\n",
            "\n",
            " Step: 0 Loss: 0.40090247988700867\n",
            "Total Training accuracy so far: 0.820\n",
            "Percent: [----------] 4.166666666666666% \n",
            " gradient 3.424109e-05\n",
            "\n",
            " Step: 50 Loss: 0.3523416221141815\n",
            "Total Training accuracy so far: 0.889\n",
            "Percent: [#---------] 8.333333333333332% \n",
            " gradient -0.00013592129\n",
            "\n",
            " Step: 100 Loss: 0.3513292372226715\n",
            "Total Training accuracy so far: 0.887\n",
            "Percent: [#---------] 12.5% \n",
            " gradient 0.00011339769\n",
            "\n",
            " Step: 150 Loss: 0.3687340319156647\n",
            "Total Training accuracy so far: 0.883\n",
            "Percent: [##--------] 16.666666666666664% \n",
            " gradient -4.341518e-05\n",
            "\n",
            " Step: 200 Loss: 0.37961673736572266\n",
            "Total Training accuracy so far: 0.878\n",
            "Percent: [##--------] 20.833333333333336% \n",
            " gradient 5.456594e-05\n",
            "\n",
            " Step: 250 Loss: 0.37744778394699097\n",
            "Total Training accuracy so far: 0.878\n",
            "Percent: [##--------] 25.0% \n",
            " gradient -2.0150044e-05\n",
            "\n",
            " Step: 300 Loss: 0.3836289346218109\n",
            "Total Training accuracy so far: 0.876\n",
            "Percent: [###-------] 29.166666666666668% \n",
            " gradient 9.173576e-05\n",
            "\n",
            " Step: 350 Loss: 0.3824024200439453\n",
            "Total Training accuracy so far: 0.875\n",
            "Percent: [###-------] 33.33333333333333% \n",
            " gradient 5.314935e-05\n",
            "\n",
            " Step: 400 Loss: 0.37754589319229126\n",
            "Total Training accuracy so far: 0.877\n",
            "Percent: [####------] 37.5% \n",
            " gradient -6.082123e-05\n",
            "\n",
            " Step: 450 Loss: 0.3739077150821686\n",
            "Total Training accuracy so far: 0.879\n",
            "Percent: [####------] 41.66666666666667% \n",
            " gradient -8.862609e-05\n",
            "\n",
            " Step: 500 Loss: 0.37087124586105347\n",
            "Total Training accuracy so far: 0.879\n",
            "Percent: [#####-----] 45.83333333333333% \n",
            " gradient 0.00012014171\n",
            "\n",
            " Step: 550 Loss: 0.3675763010978699\n",
            "Total Training accuracy so far: 0.880\n",
            "Percent: [#####-----] 50.0% \n",
            " gradient 1.63107e-05\n",
            "\n",
            " Step: 600 Loss: 0.3659936785697937\n",
            "Total Training accuracy so far: 0.880\n",
            "Percent: [#####-----] 54.166666666666664% \n",
            " gradient 0.00011892151\n",
            "\n",
            " Step: 650 Loss: 0.36697834730148315\n",
            "Total Training accuracy so far: 0.880\n",
            "Percent: [######----] 58.333333333333336% \n",
            " gradient -0.00018384248\n",
            "\n",
            " Step: 700 Loss: 0.36307981610298157\n",
            "Total Training accuracy so far: 0.881\n",
            "Percent: [######----] 62.5% \n",
            " gradient -0.00018943049\n",
            "\n",
            " Step: 750 Loss: 0.360820472240448\n",
            "Total Training accuracy so far: 0.882\n",
            "Percent: [#######---] 66.66666666666666% \n",
            " gradient -9.5798896e-05\n",
            "\n",
            " Step: 800 Loss: 0.3599856197834015\n",
            "Total Training accuracy so far: 0.883\n",
            "Percent: [#######---] 70.83333333333334% \n",
            " gradient -0.00023322417\n",
            "\n",
            " Step: 850 Loss: 0.3587360978126526\n",
            "Total Training accuracy so far: 0.883\n",
            "Percent: [########--] 75.0% \n",
            " gradient -0.00017050642\n",
            "\n",
            " Step: 900 Loss: 0.3579406440258026\n",
            "Total Training accuracy so far: 0.884\n",
            "Percent: [########--] 79.16666666666666% \n",
            " gradient -0.00012613533\n",
            "\n",
            " Step: 950 Loss: 0.3588656783103943\n",
            "Total Training accuracy so far: 0.883\n",
            "Percent: [########--] 83.33333333333334% \n",
            " gradient -5.3430464e-05\n",
            "\n",
            " Step: 1000 Loss: 0.35904136300086975\n",
            "Total Training accuracy so far: 0.883\n",
            "Percent: [#########-] 87.5% \n",
            " gradient -2.0457157e-05\n",
            "\n",
            " Step: 1050 Loss: 0.357928991317749\n",
            "Total Training accuracy so far: 0.883\n",
            "Percent: [#########-] 91.66666666666666% \n",
            " gradient -9.794287e-05\n",
            "\n",
            " Step: 1100 Loss: 0.3573719561100006\n",
            "Total Training accuracy so far: 0.883\n",
            "Percent: [##########] 95.83333333333334% \n",
            " gradient 0.00016763214\n",
            "\n",
            " Step: 1150 Loss: 0.35467982292175293\n",
            "Total Training accuracy so far: 0.884\n",
            "Percent: [##########] 99.91666666666667% Training Acc   0.8848187327384949\n",
            "Training error   0.3510104715824127\n",
            "Percent: [----------] 0.0% Step: 0 Loss: 0.21726062893867493\n",
            "Total validation accuracy so far: 0.940\n",
            "Percent: [##--------] 25.0% Step: 50 Loss: 0.18361276388168335\n",
            "Total validation accuracy so far: 0.920\n",
            "Percent: [#####-----] 50.0% Step: 100 Loss: 0.039024919271469116\n",
            "Total validation accuracy so far: 0.980\n",
            "Percent: [########--] 75.0% Step: 150 Loss: 0.11503168195486069\n",
            "Total validation accuracy so far: 0.960\n",
            "Percent: [##########] 99.5% Total Training Time:  745.742781167\n",
            "Training Acc   0.8848187327384949\n",
            "Validation Acc   0.9375003576278687\n",
            "------------------------------------\n",
            "Training error   0.3510104715824127\n",
            "Validation error   0.21081960201263428\n",
            "Epoch:  5 / 5\n",
            "Percent: [----------] 0.0% \n",
            " gradient -0.00013792096\n",
            "\n",
            " Step: 0 Loss: 0.34316012263298035\n",
            "Total Training accuracy so far: 0.860\n",
            "Percent: [----------] 4.166666666666666% \n",
            " gradient 0.00015557397\n",
            "\n",
            " Step: 50 Loss: 0.31242382526397705\n",
            "Total Training accuracy so far: 0.899\n",
            "Percent: [#---------] 8.333333333333332% \n",
            " gradient 3.888662e-05\n",
            "\n",
            " Step: 100 Loss: 0.28812214732170105\n",
            "Total Training accuracy so far: 0.905\n",
            "Percent: [#---------] 12.5% \n",
            " gradient 2.4060718e-08\n",
            "\n",
            " Step: 150 Loss: 0.308640718460083\n",
            "Total Training accuracy so far: 0.901\n",
            "Percent: [##--------] 16.666666666666664% \n",
            " gradient 9.229968e-05\n",
            "\n",
            " Step: 200 Loss: 0.3148183226585388\n",
            "Total Training accuracy so far: 0.898\n",
            "Percent: [##--------] 20.833333333333336% \n",
            " gradient -8.674223e-05\n",
            "\n",
            " Step: 250 Loss: 0.31642287969589233\n",
            "Total Training accuracy so far: 0.897\n",
            "Percent: [##--------] 25.0% \n",
            " gradient 9.344507e-05\n",
            "\n",
            " Step: 300 Loss: 0.3232388198375702\n",
            "Total Training accuracy so far: 0.893\n",
            "Percent: [###-------] 29.166666666666668% \n",
            " gradient -0.00014926305\n",
            "\n",
            " Step: 350 Loss: 0.32165685296058655\n",
            "Total Training accuracy so far: 0.893\n",
            "Percent: [###-------] 33.33333333333333% \n",
            " gradient 5.263704e-05\n",
            "\n",
            " Step: 400 Loss: 0.3194841146469116\n",
            "Total Training accuracy so far: 0.894\n",
            "Percent: [####------] 37.5% \n",
            " gradient 8.002575e-05\n",
            "\n",
            " Step: 450 Loss: 0.31703177094459534\n",
            "Total Training accuracy so far: 0.895\n",
            "Percent: [####------] 41.66666666666667% \n",
            " gradient -7.390799e-05\n",
            "\n",
            " Step: 500 Loss: 0.3147867023944855\n",
            "Total Training accuracy so far: 0.896\n",
            "Percent: [#####-----] 45.83333333333333% \n",
            " gradient -3.0510353e-05\n",
            "\n",
            " Step: 550 Loss: 0.31317323446273804\n",
            "Total Training accuracy so far: 0.897\n",
            "Percent: [#####-----] 50.0% \n",
            " gradient 0.00015075212\n",
            "\n",
            " Step: 600 Loss: 0.31264039874076843\n",
            "Total Training accuracy so far: 0.897\n",
            "Percent: [#####-----] 54.166666666666664% \n",
            " gradient 7.421025e-05\n",
            "\n",
            " Step: 650 Loss: 0.31377285718917847\n",
            "Total Training accuracy so far: 0.896\n",
            "Percent: [######----] 58.333333333333336% \n",
            " gradient -0.0004653051\n",
            "\n",
            " Step: 700 Loss: 0.31070175766944885\n",
            "Total Training accuracy so far: 0.897\n",
            "Percent: [######----] 62.5% \n",
            " gradient 9.694763e-05\n",
            "\n",
            " Step: 750 Loss: 0.30892258882522583\n",
            "Total Training accuracy so far: 0.897\n",
            "Percent: [#######---] 66.66666666666666% \n",
            " gradient -1.2364262e-05\n",
            "\n",
            " Step: 800 Loss: 0.3090958297252655\n",
            "Total Training accuracy so far: 0.897\n",
            "Percent: [#######---] 70.83333333333334% \n",
            " gradient -0.0001349947\n",
            "\n",
            " Step: 850 Loss: 0.30747514963150024\n",
            "Total Training accuracy so far: 0.898\n",
            "Percent: [########--] 75.0% \n",
            " gradient 2.641073e-05\n",
            "\n",
            " Step: 900 Loss: 0.3066757917404175\n",
            "Total Training accuracy so far: 0.898\n",
            "Percent: [########--] 79.16666666666666% \n",
            " gradient 1.5471645e-05\n",
            "\n",
            " Step: 950 Loss: 0.30894842743873596\n",
            "Total Training accuracy so far: 0.897\n",
            "Percent: [########--] 83.33333333333334% \n",
            " gradient -3.2901712e-06\n",
            "\n",
            " Step: 1000 Loss: 0.3094644248485565\n",
            "Total Training accuracy so far: 0.897\n",
            "Percent: [#########-] 87.5% \n",
            " gradient 3.660666e-06\n",
            "\n",
            " Step: 1050 Loss: 0.3075036406517029\n",
            "Total Training accuracy so far: 0.897\n",
            "Percent: [#########-] 91.66666666666666% \n",
            " gradient -0.00014029263\n",
            "\n",
            " Step: 1100 Loss: 0.30695226788520813\n",
            "Total Training accuracy so far: 0.898\n",
            "Percent: [##########] 95.83333333333334% \n",
            " gradient -0.00014635787\n",
            "\n",
            " Step: 1150 Loss: 0.3053188920021057\n",
            "Total Training accuracy so far: 0.898\n",
            "Percent: [##########] 99.91666666666667% Training Acc   0.8991677761077881\n",
            "Training error   0.3025326430797577\n",
            "Percent: [----------] 0.0% Step: 0 Loss: 0.13109153509140015\n",
            "Total validation accuracy so far: 0.940\n",
            "Percent: [##--------] 25.0% Step: 50 Loss: 0.15389887988567352\n",
            "Total validation accuracy so far: 0.920\n",
            "Percent: [#####-----] 50.0% Step: 100 Loss: 0.021976802498102188\n",
            "Total validation accuracy so far: 1.000\n",
            "Percent: [########--] 75.0% Step: 150 Loss: 0.10345953702926636\n",
            "Total validation accuracy so far: 0.960\n",
            "Percent: [##########] 99.5% Total Training Time:  956.8651306389997\n",
            "Training Acc   0.8991677761077881\n",
            "Validation Acc   0.9460999369621277\n",
            "------------------------------------\n",
            "Training error   0.3025326430797577\n",
            "Validation error   0.18168748915195465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "     main_function(Training=False , Testing=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwyBJ8WpubhM",
        "outputId": "6893696e-7bfa-4f01-d7f2-c9d50cfe5ffc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percent: [----------] 0.0% Total running accuracy so far: 0.940\n",
            "Percent: [#####-----] 50.0% Total running accuracy so far: 1.000\n",
            "Percent: [##########] 99.5% \n",
            "Test accuracy :  0.9460999989509582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2fW_nKX1ubQ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}